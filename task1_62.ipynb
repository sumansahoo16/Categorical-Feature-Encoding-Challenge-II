{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task1_62.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMjm5yt8nzy7+klSoXnZtWd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumansahoo16/Categorical-Feature-Encoding-Challenge-II/blob/master/task1_62.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPYDIooXtSbD",
        "outputId": "42b4d02b-9edc-4ff1-9c36-5b045f18e6f8"
      },
      "source": [
        "#To access files from google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nplNIWGiugvC"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def seed_everything(seed = 16):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False # set True to be faster\n",
        "seed_everything()\n",
        "\n",
        "class cfg:\n",
        "  seed = 43\n",
        "  batch_size = 32\n",
        "  num_workers = 2\n",
        "\n",
        "  EPOCHS = 100\n",
        "\n",
        "  LR = 0.001\n",
        "  weight_decay = 0.0\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PiOGgwS_wui"
      },
      "source": [
        "def get_data():\n",
        "  \"\"\"\n",
        "  Arranges Image paths and Labels in pandas DataFrame\n",
        "  \"\"\"\n",
        "  files = glob.glob('train/*/*.png')\n",
        "\n",
        "  data = pd.DataFrame()\n",
        "  data['image'] = files\n",
        "  data['label'] = data['image'].apply(lambda x : int(x[12:15]) -1)\n",
        "\n",
        "  return data"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4LVdvfbBErR"
      },
      "source": [
        "def rotate_image(image, angle = 5.0):\n",
        "  image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
        "  rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
        "  result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
        "  return result \n",
        "  \n",
        "class DataSet(Dataset):\n",
        "  def __init__(self, df, rotate = False):\n",
        "    self.df = df \n",
        "    self.rotate = rotate\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.df.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    row = self.df.loc[idx]\n",
        "\n",
        "    img = cv2.imread(row.image)\n",
        "\n",
        "    if self.rotate:\n",
        "      img = rotate_image(img, random.uniform(-5.0, 5.0))\n",
        "\n",
        "    #Removing some White Space from images\n",
        "    img = img[200:1000, 100:800]\n",
        "\n",
        "    #img = cv2.ximgproc.thinning(img)\n",
        "\n",
        "    #Resizing Image\n",
        "    img = cv2.resize(img, (28,28))\n",
        "\n",
        "    #Normalizing Images\n",
        "    img = img / 255\n",
        "\n",
        "    #Pytorch Accepts [bs, channels, h, b]\n",
        "    img = img.transpose(2,0,1)\n",
        "\n",
        "    #Only one channel\n",
        "    img = img[0:1, :, :]\n",
        "\n",
        "    return torch.tensor(img).float(), torch.tensor(row.label).long()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3FLWaPGEtXy"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d( 1, 64, 3, 1, 2)\n",
        "        self.conv2 = nn.Conv2d(64, 32, 3)\n",
        "        self.conv3 = nn.Conv2d(32, 16, 3)\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "        self.dropout = nn.Dropout2d(0.1)\n",
        "\n",
        "        self.out = nn.Linear(16 * 2 * 2, 62)\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "        x = F.relu(self.conv1(x))\n",
        "        #print(x.shape)\n",
        "        x = self.maxpool(x)\n",
        "        #print(x.shape)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        #print(x.shape)\n",
        "        x = self.maxpool(x)\n",
        "        #print(x.shape)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        #print(x.shape)\n",
        "        x = self.maxpool(x)\n",
        "        #print(x.shape)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        #print(x.shape)\n",
        "        x = self.out(x)\n",
        "        #print(x.shape)\n",
        "\n",
        "        return x\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9BD4Z2MPBbz"
      },
      "source": [
        "###################################################################################################\n",
        "def train_func(model, data_loader, criterion, optimizer, scheduler = None, device = torch.device(\"cuda\")):\n",
        "    train_labels, train_preds = [], []\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    model.train()\n",
        "    for step, (images, labels) in enumerate(data_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        y_preds = model(images)\n",
        "        \n",
        "        loss = criterion(y_preds, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "        train_preds.append(y_preds.softmax(1).to('cpu').detach().numpy())\n",
        "        train_labels.append(labels.to('cpu').numpy())\n",
        "        \n",
        "    return train_loss / len(data_loader), np.concatenate(train_preds) , np.concatenate(train_labels) \n",
        "###################################################################################################\n",
        "\n",
        "###################################################################################################    \n",
        "def valid_func(model, data_loader, criterion, device = torch.device(\"cuda\")):\n",
        "    valid_preds =  []\n",
        "    valid_loss = 0.0\n",
        "    \n",
        "    model.eval()\n",
        "    for step, (images, labels) in enumerate(data_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            y_preds = model(images)\n",
        "        \n",
        "        loss = criterion(y_preds, labels)\n",
        "        valid_loss += loss.item()\n",
        "        \n",
        "        valid_preds.append(y_preds.softmax(1).to('cpu').detach().numpy())\n",
        "        \n",
        "    return valid_loss / len(data_loader), np.concatenate(valid_preds) \n",
        "###################################################################################################"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMHRl7NE-SsJ"
      },
      "source": [
        "def main():\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    #Unzip Task 1 images\n",
        "    #!unzip -qq  gdrive/MyDrive/trainPart1.zip\n",
        "\n",
        "    data = get_data()\n",
        "\n",
        "    train, valid = train_test_split(data, test_size = 0.2, random_state = cfg.seed, stratify = data['label'])\n",
        "\n",
        "    train = train.reset_index()\n",
        "    valid = valid.reset_index()\n",
        "\n",
        "    train_dataset = DataSet(train, rotate = False)\n",
        "    valid_dataset = DataSet(valid)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size = cfg.batch_size, \n",
        "                          shuffle = True, num_workers = cfg.num_workers,\n",
        "                          pin_memory = True, drop_last = True)\n",
        "    \n",
        "    valid_loader = DataLoader(valid_dataset, batch_size = cfg.batch_size, \n",
        "                          shuffle = False, num_workers = cfg.num_workers,\n",
        "                          pin_memory = True, drop_last = False)\n",
        "\n",
        "    model = Model()\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(),lr = cfg.LR,\n",
        "                                weight_decay = cfg.weight_decay)\n",
        "    \n",
        "    #scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5, 50, 60], gamma=0.1)\n",
        "   \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    #-----------------------------------------------------------------#\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    \n",
        "    train_acc = []\n",
        "    valid_acc = []\n",
        "    \n",
        "    for epoch in range(cfg.EPOCHS):\n",
        "        start_time = time.time()\n",
        "        \n",
        "        train_loss, train_prediction, train_labels = train_func(model, train_loader, criterion, optimizer)\n",
        "        valid_loss, valid_prediction = valid_func(model, valid_loader, criterion)\n",
        "        train_losses.append(train_loss)\n",
        "        valid_losses.append(valid_loss)\n",
        "        \n",
        "        train_acc_ = accuracy_score(train_labels, np.argmax(train_prediction, axis = 1))\n",
        "        valid_acc_ = accuracy_score(valid.label , np.argmax(valid_prediction, axis = 1))\n",
        "        train_acc.append(train_acc_)\n",
        "        valid_acc.append(valid_acc_)\n",
        "        \n",
        "        #scheduler.step()\n",
        "        \n",
        "        \n",
        "        #torch.save(model.state_dict(), f'{cfg.model_name}_{cfg.training_fold}_{epoch}.pth')\n",
        "        time_taken = time.time() - start_time\n",
        "        print('Epoch {:2d} | loss: {:.4f}  | val_Loss: {:.4f}  | Acc : {:.3f}  | Val_Acc : {:.3f}  | {:d}s'.\n",
        "          format(epoch, train_loss, valid_loss, train_acc_, valid_acc_, int(time_taken)))\n",
        "        \n",
        "    return train_losses, valid_losses, train_acc, valid_acc\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDVrl7ZS_mW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a6559f4-64dc-4079-9d5e-345c813391e8"
      },
      "source": [
        "fold0 = main() "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0 | loss: 4.1326  | val_Loss: 4.1271  | Acc : 0.019  | Val_Acc : 0.026  | 13s\n",
            "Epoch  1 | loss: 4.1207  | val_Loss: 4.0908  | Acc : 0.023  | Val_Acc : 0.036  | 13s\n",
            "Epoch  2 | loss: 3.9778  | val_Loss: 3.7126  | Acc : 0.052  | Val_Acc : 0.101  | 14s\n",
            "Epoch  3 | loss: 3.4843  | val_Loss: 3.2379  | Acc : 0.134  | Val_Acc : 0.177  | 13s\n",
            "Epoch  4 | loss: 3.0897  | val_Loss: 2.9830  | Acc : 0.198  | Val_Acc : 0.220  | 13s\n",
            "Epoch  5 | loss: 2.8763  | val_Loss: 2.7631  | Acc : 0.253  | Val_Acc : 0.280  | 13s\n",
            "Epoch  6 | loss: 2.6680  | val_Loss: 2.6525  | Acc : 0.302  | Val_Acc : 0.306  | 13s\n",
            "Epoch  7 | loss: 2.5490  | val_Loss: 2.5316  | Acc : 0.343  | Val_Acc : 0.337  | 13s\n",
            "Epoch  8 | loss: 2.4007  | val_Loss: 2.4410  | Acc : 0.369  | Val_Acc : 0.341  | 13s\n",
            "Epoch  9 | loss: 2.3140  | val_Loss: 2.3742  | Acc : 0.386  | Val_Acc : 0.343  | 13s\n",
            "Epoch 10 | loss: 2.2002  | val_Loss: 2.3269  | Acc : 0.421  | Val_Acc : 0.385  | 13s\n",
            "Epoch 11 | loss: 2.1411  | val_Loss: 2.2817  | Acc : 0.423  | Val_Acc : 0.373  | 13s\n",
            "Epoch 12 | loss: 2.0494  | val_Loss: 2.2174  | Acc : 0.452  | Val_Acc : 0.385  | 13s\n",
            "Epoch 13 | loss: 2.0040  | val_Loss: 2.1957  | Acc : 0.444  | Val_Acc : 0.411  | 13s\n",
            "Epoch 14 | loss: 1.9467  | val_Loss: 2.1488  | Acc : 0.465  | Val_Acc : 0.421  | 13s\n",
            "Epoch 15 | loss: 1.8737  | val_Loss: 2.1205  | Acc : 0.480  | Val_Acc : 0.423  | 13s\n",
            "Epoch 16 | loss: 1.8068  | val_Loss: 2.0971  | Acc : 0.506  | Val_Acc : 0.427  | 13s\n",
            "Epoch 17 | loss: 1.7830  | val_Loss: 2.0559  | Acc : 0.503  | Val_Acc : 0.438  | 13s\n",
            "Epoch 18 | loss: 1.7551  | val_Loss: 2.0634  | Acc : 0.515  | Val_Acc : 0.454  | 13s\n",
            "Epoch 19 | loss: 1.6915  | val_Loss: 2.0510  | Acc : 0.532  | Val_Acc : 0.458  | 13s\n",
            "Epoch 20 | loss: 1.6688  | val_Loss: 2.0352  | Acc : 0.534  | Val_Acc : 0.468  | 13s\n",
            "Epoch 21 | loss: 1.6222  | val_Loss: 1.9689  | Acc : 0.538  | Val_Acc : 0.480  | 13s\n",
            "Epoch 22 | loss: 1.5707  | val_Loss: 1.9857  | Acc : 0.572  | Val_Acc : 0.476  | 13s\n",
            "Epoch 23 | loss: 1.5365  | val_Loss: 1.9863  | Acc : 0.555  | Val_Acc : 0.488  | 13s\n",
            "Epoch 24 | loss: 1.5063  | val_Loss: 1.9724  | Acc : 0.573  | Val_Acc : 0.490  | 13s\n",
            "Epoch 25 | loss: 1.5352  | val_Loss: 1.9352  | Acc : 0.562  | Val_Acc : 0.470  | 13s\n",
            "Epoch 26 | loss: 1.4724  | val_Loss: 1.9441  | Acc : 0.571  | Val_Acc : 0.476  | 13s\n",
            "Epoch 27 | loss: 1.4395  | val_Loss: 1.9140  | Acc : 0.581  | Val_Acc : 0.480  | 13s\n",
            "Epoch 28 | loss: 1.4120  | val_Loss: 1.9559  | Acc : 0.589  | Val_Acc : 0.476  | 13s\n",
            "Epoch 29 | loss: 1.3696  | val_Loss: 1.9357  | Acc : 0.596  | Val_Acc : 0.494  | 13s\n",
            "Epoch 30 | loss: 1.3828  | val_Loss: 1.9311  | Acc : 0.604  | Val_Acc : 0.478  | 13s\n",
            "Epoch 31 | loss: 1.3320  | val_Loss: 1.9408  | Acc : 0.617  | Val_Acc : 0.484  | 13s\n",
            "Epoch 32 | loss: 1.3124  | val_Loss: 1.9095  | Acc : 0.619  | Val_Acc : 0.486  | 13s\n",
            "Epoch 33 | loss: 1.2889  | val_Loss: 1.9038  | Acc : 0.616  | Val_Acc : 0.506  | 13s\n",
            "Epoch 34 | loss: 1.2964  | val_Loss: 1.9060  | Acc : 0.621  | Val_Acc : 0.498  | 13s\n",
            "Epoch 35 | loss: 1.3031  | val_Loss: 1.8974  | Acc : 0.611  | Val_Acc : 0.504  | 13s\n",
            "Epoch 36 | loss: 1.2685  | val_Loss: 1.9292  | Acc : 0.622  | Val_Acc : 0.508  | 13s\n",
            "Epoch 37 | loss: 1.2461  | val_Loss: 1.9211  | Acc : 0.628  | Val_Acc : 0.516  | 13s\n",
            "Epoch 38 | loss: 1.2573  | val_Loss: 1.9504  | Acc : 0.627  | Val_Acc : 0.500  | 13s\n",
            "Epoch 39 | loss: 1.2417  | val_Loss: 1.9096  | Acc : 0.645  | Val_Acc : 0.522  | 13s\n",
            "Epoch 40 | loss: 1.1994  | val_Loss: 1.9244  | Acc : 0.650  | Val_Acc : 0.506  | 13s\n",
            "Epoch 41 | loss: 1.2219  | val_Loss: 1.9088  | Acc : 0.636  | Val_Acc : 0.524  | 13s\n",
            "Epoch 42 | loss: 1.1760  | val_Loss: 1.8974  | Acc : 0.645  | Val_Acc : 0.518  | 13s\n",
            "Epoch 43 | loss: 1.1526  | val_Loss: 1.9295  | Acc : 0.661  | Val_Acc : 0.550  | 13s\n",
            "Epoch 44 | loss: 1.1662  | val_Loss: 1.8937  | Acc : 0.648  | Val_Acc : 0.536  | 13s\n",
            "Epoch 45 | loss: 1.1640  | val_Loss: 1.9122  | Acc : 0.653  | Val_Acc : 0.544  | 13s\n",
            "Epoch 46 | loss: 1.1016  | val_Loss: 1.9246  | Acc : 0.674  | Val_Acc : 0.534  | 13s\n",
            "Epoch 47 | loss: 1.1548  | val_Loss: 1.9019  | Acc : 0.671  | Val_Acc : 0.528  | 13s\n",
            "Epoch 48 | loss: 1.1383  | val_Loss: 1.9472  | Acc : 0.661  | Val_Acc : 0.562  | 13s\n",
            "Epoch 49 | loss: 1.0899  | val_Loss: 1.9475  | Acc : 0.683  | Val_Acc : 0.548  | 13s\n",
            "Epoch 50 | loss: 1.1139  | val_Loss: 1.9470  | Acc : 0.664  | Val_Acc : 0.540  | 13s\n",
            "Epoch 51 | loss: 1.1262  | val_Loss: 1.9688  | Acc : 0.664  | Val_Acc : 0.536  | 13s\n",
            "Epoch 52 | loss: 1.0720  | val_Loss: 1.9355  | Acc : 0.683  | Val_Acc : 0.546  | 13s\n",
            "Epoch 53 | loss: 1.0853  | val_Loss: 1.9335  | Acc : 0.666  | Val_Acc : 0.540  | 13s\n",
            "Epoch 54 | loss: 1.0922  | val_Loss: 1.9376  | Acc : 0.682  | Val_Acc : 0.554  | 13s\n",
            "Epoch 55 | loss: 1.0607  | val_Loss: 1.9395  | Acc : 0.672  | Val_Acc : 0.546  | 13s\n",
            "Epoch 56 | loss: 1.0413  | val_Loss: 1.9116  | Acc : 0.698  | Val_Acc : 0.552  | 13s\n",
            "Epoch 57 | loss: 1.0093  | val_Loss: 1.9931  | Acc : 0.699  | Val_Acc : 0.550  | 13s\n",
            "Epoch 58 | loss: 1.0260  | val_Loss: 1.9180  | Acc : 0.686  | Val_Acc : 0.556  | 13s\n",
            "Epoch 59 | loss: 1.0037  | val_Loss: 1.9221  | Acc : 0.700  | Val_Acc : 0.558  | 13s\n",
            "Epoch 60 | loss: 0.9979  | val_Loss: 1.9484  | Acc : 0.694  | Val_Acc : 0.550  | 13s\n",
            "Epoch 61 | loss: 0.9986  | val_Loss: 1.9568  | Acc : 0.696  | Val_Acc : 0.540  | 13s\n",
            "Epoch 62 | loss: 0.9949  | val_Loss: 1.9105  | Acc : 0.700  | Val_Acc : 0.548  | 13s\n",
            "Epoch 63 | loss: 0.9930  | val_Loss: 1.9684  | Acc : 0.690  | Val_Acc : 0.546  | 13s\n",
            "Epoch 64 | loss: 0.9804  | val_Loss: 1.9519  | Acc : 0.694  | Val_Acc : 0.562  | 13s\n",
            "Epoch 65 | loss: 0.9875  | val_Loss: 1.9324  | Acc : 0.712  | Val_Acc : 0.552  | 13s\n",
            "Epoch 66 | loss: 0.9913  | val_Loss: 1.9337  | Acc : 0.705  | Val_Acc : 0.546  | 13s\n",
            "Epoch 67 | loss: 0.9352  | val_Loss: 1.9296  | Acc : 0.708  | Val_Acc : 0.567  | 13s\n",
            "Epoch 68 | loss: 0.9399  | val_Loss: 1.9414  | Acc : 0.721  | Val_Acc : 0.556  | 13s\n",
            "Epoch 69 | loss: 0.9474  | val_Loss: 1.9691  | Acc : 0.707  | Val_Acc : 0.542  | 13s\n",
            "Epoch 70 | loss: 0.9459  | val_Loss: 1.9616  | Acc : 0.706  | Val_Acc : 0.546  | 13s\n",
            "Epoch 71 | loss: 0.9653  | val_Loss: 1.9921  | Acc : 0.707  | Val_Acc : 0.544  | 13s\n",
            "Epoch 72 | loss: 0.9166  | val_Loss: 1.9639  | Acc : 0.722  | Val_Acc : 0.556  | 13s\n",
            "Epoch 73 | loss: 0.8964  | val_Loss: 1.9899  | Acc : 0.717  | Val_Acc : 0.558  | 13s\n",
            "Epoch 74 | loss: 0.9277  | val_Loss: 1.9630  | Acc : 0.717  | Val_Acc : 0.550  | 13s\n",
            "Epoch 75 | loss: 0.9026  | val_Loss: 1.9331  | Acc : 0.711  | Val_Acc : 0.565  | 13s\n",
            "Epoch 76 | loss: 0.8958  | val_Loss: 2.0064  | Acc : 0.720  | Val_Acc : 0.558  | 13s\n",
            "Epoch 77 | loss: 0.9042  | val_Loss: 1.9929  | Acc : 0.728  | Val_Acc : 0.536  | 13s\n",
            "Epoch 78 | loss: 0.9523  | val_Loss: 1.9620  | Acc : 0.712  | Val_Acc : 0.550  | 13s\n",
            "Epoch 79 | loss: 0.9304  | val_Loss: 1.9762  | Acc : 0.712  | Val_Acc : 0.544  | 13s\n",
            "Epoch 80 | loss: 0.8958  | val_Loss: 1.9721  | Acc : 0.738  | Val_Acc : 0.575  | 13s\n",
            "Epoch 81 | loss: 0.8992  | val_Loss: 1.9567  | Acc : 0.722  | Val_Acc : 0.556  | 13s\n",
            "Epoch 82 | loss: 0.8911  | val_Loss: 1.9629  | Acc : 0.729  | Val_Acc : 0.562  | 13s\n",
            "Epoch 83 | loss: 0.8818  | val_Loss: 1.9679  | Acc : 0.724  | Val_Acc : 0.560  | 13s\n",
            "Epoch 84 | loss: 0.8907  | val_Loss: 2.0248  | Acc : 0.723  | Val_Acc : 0.569  | 13s\n",
            "Epoch 85 | loss: 0.8433  | val_Loss: 1.9875  | Acc : 0.745  | Val_Acc : 0.567  | 13s\n",
            "Epoch 86 | loss: 0.8820  | val_Loss: 2.0038  | Acc : 0.720  | Val_Acc : 0.550  | 13s\n",
            "Epoch 87 | loss: 0.8794  | val_Loss: 1.9825  | Acc : 0.741  | Val_Acc : 0.556  | 13s\n",
            "Epoch 88 | loss: 0.8565  | val_Loss: 1.9882  | Acc : 0.729  | Val_Acc : 0.558  | 13s\n",
            "Epoch 89 | loss: 0.8556  | val_Loss: 2.0220  | Acc : 0.730  | Val_Acc : 0.569  | 13s\n",
            "Epoch 90 | loss: 0.8535  | val_Loss: 2.0389  | Acc : 0.742  | Val_Acc : 0.554  | 13s\n",
            "Epoch 91 | loss: 0.8518  | val_Loss: 1.9788  | Acc : 0.735  | Val_Acc : 0.552  | 13s\n",
            "Epoch 92 | loss: 0.8515  | val_Loss: 2.0441  | Acc : 0.735  | Val_Acc : 0.565  | 13s\n",
            "Epoch 93 | loss: 0.8314  | val_Loss: 2.0094  | Acc : 0.744  | Val_Acc : 0.556  | 13s\n",
            "Epoch 94 | loss: 0.8084  | val_Loss: 2.0481  | Acc : 0.753  | Val_Acc : 0.562  | 13s\n",
            "Epoch 95 | loss: 0.8267  | val_Loss: 2.0325  | Acc : 0.746  | Val_Acc : 0.569  | 13s\n",
            "Epoch 96 | loss: 0.8609  | val_Loss: 2.0043  | Acc : 0.733  | Val_Acc : 0.558  | 13s\n",
            "Epoch 97 | loss: 0.8103  | val_Loss: 2.0282  | Acc : 0.746  | Val_Acc : 0.567  | 13s\n",
            "Epoch 98 | loss: 0.8394  | val_Loss: 2.0495  | Acc : 0.741  | Val_Acc : 0.556  | 13s\n",
            "Epoch 99 | loss: 0.8300  | val_Loss: 1.9986  | Acc : 0.746  | Val_Acc : 0.575  | 13s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}